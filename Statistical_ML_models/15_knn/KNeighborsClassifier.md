<div style="margin-left: 20px; margin-right: 20px;">

# ðŸ“˜ **KNeighborsClassifier â€“ Parameters Explained (with Real-World Examples)**

```python
from sklearn.neighbors import KNeighborsClassifier
```

---

# 1. **n_neighbors**

### âœ” What it means

Number of nearest neighbors (K) used to classify a new data point.

### âœ” Why use it

Controls how many neighbors vote for the final class.

* Small K â†’ more flexible, may overfit
* Large K â†’ more stable, may underfit

### âœ” Real-world Example

Classifying whether a fruit is **apple or orange** based on its weight + color.

* With **K=1**, the closest fruit decides the class â†’ very sensitive
* With **K=10**, decision is smoother

### âœ” Typical Range / Values

```
1â€“50
(Most common: 3, 5, 7, 9)
```

---

# 2. **weights**

### âœ” What it means

How the influence of neighbors is calculated.

Values:

* **"uniform"** â†’ all neighbors have equal vote
* **"distance"** â†’ closer neighbors have stronger vote
* **callable** â†’ user-defined weighting function

### âœ” Why use it

Improves accuracy when data points closer to the target matter more.

### âœ” Real-world Example

Predicting if a credit card transaction is **fraudulent**.

* A very close similar transaction is more important â†’ `weights="distance"`

### âœ” Typical Values

```
"uniform" (default)
"distance"
custom function
```

---

# 3. **algorithm**

### âœ” What it means

Which algorithm is used to find nearest neighbors.

Options:

* **"auto"** â€“ chooses best automatically
* **"ball_tree"** â€“ good for high dimensional
* **"kd_tree"** â€“ good for medium dimensional
* **"brute"** â€“ slow but simple (distance to all points)

### âœ” Why use it

Improves search speed for large datasets.

### âœ” Real-world Example

Face recognition system:

* High dimensions (128â€“2048 embedding vector)
* **ball_tree** or **brute** works better

### âœ” Typical Range / Values

```
"auto" (most common)
"brute" (for high dimensions)
"ball_tree" 
"kd_tree"
```

---

# 4. **leaf_size**

### âœ” What it means

Affects the speed/memory for BallTree / KDTree operations.

### âœ” Why use it

Smaller leaf size = deeper tree (slower query)
Larger leaf size = shallower tree (faster query)

### âœ” Real-world Example

Large e-commerce dataset recommending similar products:

* Increasing leaf size improves performance for millions of products

### âœ” Typical Range

```
20â€“100
(Default = 30)
```

---

# 5. **p (Minkowski metric power)**

### âœ” What it means

Defines the distance measure:

* **p = 1** â†’ Manhattan distance
* **p = 2** â†’ Euclidean distance (most used)
* **p > 2** â†’ increasingly large penalty for bigger differences

### âœ” Why use it

Choosing the right distance metric can drastically improve accuracy.

### âœ” Real-world Example

Recommender system comparing user rating patterns:

* Manhattan distance (p=1) often performs better when user vectors are sparse.

### âœ” Typical Values

```
1 (L1)
2 (L2) â€“ default
3â€“5 (rare)
```

---

# 6. **metric**

### âœ” What it means

Which distance formula to use (string or function).

Common values:

* **"minkowski"** â†’ controlled by p
* **"euclidean"**
* **"manhattan"**
* **"cosine"**
* **"hamming"**

### âœ” Why use it

Different data types require different distance metrics.

### âœ” Real-world Example

Text similarity (Bag-of-Words vectors):

* Cosine distance performs better â†’ `metric="cosine"`

### âœ” Typical Values

```
"minkowski" (default)
"euclidean"
"manhattan"
"cosine"
"hamming"
custom function
```

---

# 7. **metric_params**

### âœ” What it means

Additional arguments for custom distance metrics.

### âœ” Why use it

Allows fine-tuning the distance function.

### âœ” Real-world Example

When using a custom metric that needs extra parameters, such as:

* Scale factors
* Thresholds
* Penalties

### âœ” Typical Values

```
None (most common)
{"w": [0.2, 0.5, 1.0]}   # example custom parameters
```

---

# 8. **n_jobs**

### âœ” What it means

Number of CPU cores to use in neighbor search.

* `None` â†’ 1 core
* `-1` â†’ all available cores

### âœ” Why use it

Speeds up prediction, especially for large datasets.

### âœ” Real-world Example

Medical diagnostic system:

* 300k patient records
* Setting `n_jobs = -1` reduces prediction time dramatically

### âœ” Typical Values

```
None  (default)
1â€“8   (manual control)
-1    (use all cores)
```

---

# ðŸ“Œ Summary Table

| Parameter         | Meaning                | Reason to Use         | Real-World Example        | Typical Range         |
| ----------------- | ---------------------- | --------------------- | ------------------------- | --------------------- |
| **n_neighbors**   | No. of neighbors       | Biasâ€“variance control | Classify fruit type       | 3â€“15                  |
| **weights**       | Vote weighting         | Closer points matter  | Fraud detection           | "uniform", "distance" |
| **algorithm**     | Neighbor search method | Speed & efficiency    | Face recognition          | "auto", "brute"       |
| **leaf_size**     | Tree speed/memory      | Optimize search time  | Product recommendation    | 20â€“100                |
| **p**             | Distance power         | Choose L1/L2 metric   | User rating similarity    | 1, 2                  |
| **metric**        | Distance function      | Adapt to data type    | Text similarity           | "euclidean", "cosine" |
| **metric_params** | Extra params           | Custom metrics        | Weighted custom distances | None / dict           |
| **n_jobs**        | CPU cores              | Speed up computation  | Large medical dataset     | -1, None              |

---
</div>